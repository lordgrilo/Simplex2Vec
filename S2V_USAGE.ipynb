{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from Simplex2Vec.Simplex2Vec import Simplex2Vec\n",
    "from Simplex2Vec.utils import check_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From NetworkX Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a graph\n",
    "G = nx.fast_gnp_random_graph(n=100, p=0.1, seed=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Hasse diagram and random walks\n",
    "order = 2\n",
    "s2v = Simplex2Vec(G, n_walks=15, walk_length=10, hasse_max_order=order, workers=2)  # keep p=q=1 to have the usual RW\n",
    "\n",
    "# Embed nodes\n",
    "model = s2v.fit()  # Any keywords acceptable by gensim.Word2Vec can be passed, `workers` is automatically passed from the Node2Vec constructor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look for most similar nodes\n",
    "model.wv.most_similar('2')  # Output node names are always strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Can also be made a one-liner\n",
    "model = Simplex2Vec(G, n_walks=15, walk_length=10,  workers=2).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump created hasse diagram to a pickle file\n",
    "s2v.dump_hasse_diagram('hasse_diag.pkl')\n",
    "\n",
    "### MODEL CAN BE SAVED WITH model.save(), NATIVELY FROM WORD2VEC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new Simplex2Vec from dumped hasse diagram\n",
    "s2v = Simplex2Vec.read_hasse_diagram('hasse_diag.pkl', n_walks=15)\n",
    "model = s2v.fit()\n",
    "\n",
    "model.wv.most_similar('2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create external handle for hasse diagram\n",
    "H = s2v.get_hasse_diagram(copy=False)\n",
    "\n",
    "print(H is s2v.hasse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create external deepcopy of hasse diagram\n",
    "H = s2v.get_hasse_diagram()\n",
    "\n",
    "print(H is s2v.hasse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From External Hasse Diagram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build hasse diagram externally\n",
    "\n",
    "from Simplex2Vec.utils import read_simplex_json\n",
    "from Simplex2Vec.simplex2hasse import simplex2hasse_HOexponential\n",
    "\n",
    "in_file = \"./data/sociopatterns_facets/aggr_5min_cliques_InVS15.json\"\n",
    "\n",
    "data = read_simplex_json(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g_hasse = simplex2hasse_HOexponential(data, max_order=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build model from external hasse diagram\n",
    "s2v = Simplex2Vec.from_hasse_diagram(g_hasse, n_walks=1, walk_length=10)\n",
    "model = s2v.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# limit order of imported hasse diagram\n",
    "s2v = Simplex2Vec.from_hasse_diagram(g_hasse, n_walks=1, walk_length=10, hasse_max_order=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# warning if you try to set a limit higher than what is present in the external hasse diagram\n",
    "g_hasse = simplex2hasse_HOexponential(data, max_order=2)\n",
    "print()\n",
    "\n",
    "s2v = Simplex2Vec.from_hasse_diagram(g_hasse, n_walks=1, walk_length=10, hasse_max_order=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## check higher order link prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from Simplex2Vec.Simplex2Vec import Simplex2Vec\n",
    "from Simplex2Vec.utils import check_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build hasse diagram externally\n",
    "\n",
    "from Simplex2Vec.utils import read_simplex_json\n",
    "from Simplex2Vec.simplex2hasse import simplex2hasse_HOexponential\n",
    "\n",
    "from itertools import compress\n",
    "\n",
    "in_file = \"./data/sociopatterns_facets/aggr_5min_cliques_InVS15.json\"\n",
    "'./'\n",
    "\n",
    "data = read_simplex_json(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = 0.8\n",
    "g_hasse = simplex2hasse_HOexponential(data, max_order=None)\n",
    "g_hasse_head = simplex2hasse_HOexponential(data[:int(np.round(len(data)*frac))], max_order=None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = list(g_hasse.nodes)\n",
    "hhszs = np.array([len(h) for h in hh])\n",
    "fullSetNodes = list(compress(hh,hhszs==1))\n",
    "print('Full set has ' + str(len(fullSetNodes)) + ' 0-d nodes')\n",
    "fullSetTriangles = list(compress(hh,hhszs==3))\n",
    "print('Full set has ' + str(len(fullSetTriangles)) + ' 2-d triangles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hh = list(g_hasse_head.nodes)\n",
    "hhszs = np.array([len(h) for h in hh])\n",
    "partialSetNodes = list(compress(hh,hhszs==1))\n",
    "print('Partial set has ' + str(len(partialSetNodes)) + ' 0-d nodes')\n",
    "partialSetTriangles = list(compress(hh,hhszs==3))\n",
    "print('Partial set has ' + str(len(partialSetTriangles)) + ' 2-d triangles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testSet = []\n",
    "for f in fullSetTriangles:\n",
    "    if not any(list(f <= ex for ex in partialSetTriangles)):\n",
    "        testSet.append(f)\n",
    "print(len(testSet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2v = Simplex2Vec.from_hasse_diagram(g_hasse_head, n_walks=1, walk_length=10, hasse_max_order=None)\n",
    "\n",
    "model = s2v.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_prediction(s2v,testSet,metric='euclidean')\n",
    "check_prediction(s2v,testSet,metric='cosine')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model significantly improves after further training of embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2v = Simplex2Vec.from_hasse_diagram(g_hasse_head, n_walks=10, walk_length=100, hasse_max_order=None)\n",
    "\n",
    "model = s2v.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_prediction(s2v,testSet,metric='cosine')\n",
    "\n",
    "check_prediction(s2v,testSet,metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note: The original hasse_diag graph is not modified by limiting the simplex order in the creation of the Simplex2vec object and can thus be reused."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example: Clustering \n",
    "\n",
    "The data we use here is the cluster encounters at school. Simplices are formed if students spend time together for sufficient time. The data has labels also for teachers, however we do not know which class they belong to. Thus we specifically exclude them from the data and keep track of their labels in **teachers_labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "from Simplex2Vec.Simplex2Vec import Simplex2Vec\n",
    "\n",
    "from Simplex2Vec.utils import read_simplex_json\n",
    "from Simplex2Vec.simplex2hasse import *\n",
    "\n",
    "def Data2Simplex(path):\n",
    "    data = open(path)\n",
    "    list_simplices = json.load(data)\n",
    "    list_simplices_fs=[frozenset([str(i) for i in li]) for li in list_simplices]\n",
    "    return list_simplices_fs\n",
    "\n",
    "data_dir = \"./data/sociopatterns_facets/\"\n",
    "data_file = \"aggr_10min_cliques_Thiers13.json\"\n",
    "in_file = os.path.join(data_dir, data_file)\n",
    "\n",
    "meta_data_path = \"./data/sociopatterns_meta/META_cliques_Thiers13.tsv\"\n",
    "\n",
    "data = Data2Simplex(in_file)\n",
    "\n",
    "# META information\n",
    "with open(meta_data_path, 'r') as fh:\n",
    "    f_meta = np.loadtxt(fh, dtype=str, delimiter='\\t')\n",
    "    \n",
    "teachers_labels = set([])\n",
    "for x,c in f_meta[:,[0,1]]:\n",
    "    if c == 'Teachers':\n",
    "        teachers_labels.add(x)\n",
    "\n",
    "num_students = 0\n",
    "classes_list = set([])\n",
    "for person in f_meta:\n",
    "    if person[1] != 'Teachers':\n",
    "        classes_list.add(person[1])\n",
    "        num_students += 1\n",
    "        \n",
    "num_classes = len(classes_list)\n",
    "print(\"Number of students\", num_students)\n",
    "print(\"Number of classes\", num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirement: sklearn and clusim packages . can be installed through pip.\n",
    "import clusim.sim as sim\n",
    "from clusim.clustering import Clustering\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# sample function that clusters the embedding using KMeans clustering. You may use any other clustering algorithm\n",
    "def clusterer(model, num_classes, teachers_labels = set([])):\n",
    "    X = []\n",
    "    X_labels = []\n",
    "\n",
    "    for u in model.wv.vocab.keys():\n",
    "        if len(u.split('-')) == 1 and u not in teachers_labels:\n",
    "            X.append(model[u])\n",
    "            X_labels.append(u)\n",
    "\n",
    "    clusterer = KMeans(n_clusters=num_classes)\n",
    "    pred_cluster_labels = clusterer.fit_predict(X)\n",
    "\n",
    "    X_labels_pred_clusters = []\n",
    "    predicted_clusters_dict = defaultdict(list)\n",
    "    for x,c in zip(X_labels, pred_cluster_labels):\n",
    "        predicted_clusters_dict[x].append(c)\n",
    "        X_labels_pred_clusters.append((x,c))\n",
    "\n",
    "    true_cluster_indices_dict = dict([(c,i) for i,c in enumerate(np.unique(f_meta[:,1]))])\n",
    "\n",
    "    true_clusters_dict = defaultdict(list)\n",
    "    for x, c in f_meta[:,[0,1]]:\n",
    "        if x in predicted_clusters_dict:\n",
    "            true_clusters_dict[x].append(true_cluster_indices_dict[c])\n",
    "    \n",
    "    X_labels_true_clusters = []\n",
    "    for lbl in X_labels:\n",
    "        X_labels_true_clusters.append((lbl, true_clusters_dict[lbl][0]))\n",
    "        \n",
    "    return X, X_labels_pred_clusters, X_labels_true_clusters\n",
    "\n",
    "# Output NMI - normalized mutual information of two clusterings\n",
    "def get_NMI(X_labels_pred_clusters, X_labels_true_clusters):\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    \n",
    "    true_clusters_dict = {node: [lbl] for node, lbl in X_labels_true_clusters}\n",
    "    predicted_clusters_dict = {node: [lbl] for node, lbl in X_labels_pred_clusters}\n",
    "    \n",
    "    c_true = Clustering(true_clusters_dict)\n",
    "    c_pred = Clustering(predicted_clusters_dict)\n",
    "    NMI = sim.nmi(c_pred, c_true)\n",
    "    \n",
    "    return NMI\n",
    "\n",
    "# find relabelling of the predicted clusters, that match the true labels with max accuracy\n",
    "def get_max_accuracy_labels(X_labels_pred_clusters, X_labels_true_clusters):\n",
    "    from tqdm import tqdm\n",
    "    def accuracy_score(y_pred, y_true):\n",
    "        score = 0\n",
    "        for i in range(len(y_pred)):\n",
    "            if y_pred[i] == y_true[i]:\n",
    "                score += 1\n",
    "        score /= float(len(y_pred))\n",
    "        return score\n",
    "    \n",
    "    sorted_predicted_clusters = sorted(X_labels_pred_clusters, key = lambda x: x[0])\n",
    "    predicted_clusters = [v[1] for v in sorted_predicted_clusters]\n",
    "    \n",
    "    sorted_true_clusters = sorted(X_labels_true_clusters, key = lambda x: x[0])\n",
    "    true_clusters = [v[1] for v in sorted_true_clusters]\n",
    "    sorted_labels = [v[0] for v in sorted_true_clusters]\n",
    "    \n",
    "    max_accuracy = 0\n",
    "    max_accuracy_clusters = []\n",
    "    label_permutations_list = list(itertools.permutations(range(max(true_clusters)+1)))\n",
    "    for perm_map in tqdm(label_permutations_list, desc = \"Max accuracy\"):\n",
    "        predicted_clusters_temp = []\n",
    "        for i in predicted_clusters:\n",
    "            predicted_clusters_temp.append(perm_map[i])\n",
    "        if max_accuracy < accuracy_score(predicted_clusters_temp, true_clusters):\n",
    "            max_accuracy = accuracy_score(predicted_clusters_temp, true_clusters)\n",
    "            max_accuracy_clusters.clear()\n",
    "            max_accuracy_clusters = predicted_clusters_temp\n",
    "            \n",
    "    max_acc_dict = {x: max_accuracy_clusters[i] for i, x in enumerate(sorted_labels)}\n",
    "    return max_acc_dict\n",
    "\n",
    "# plot graphically the embedding\n",
    "def plot_clusterings(X, X_labels_pred_clusters, X_labels_true_clusters, teachers_labels = set([]),\n",
    "                     max_accuracy_dict = {}):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib import cm\n",
    "    \n",
    "    X_transform = TSNE(n_components=2).fit_transform(X)\n",
    "    X_labels = [node for node, lbl in X_labels_pred_clusters]\n",
    "    if len(max_accuracy_dict):\n",
    "        cluster_labels_pred = [max_accuracy_dict[node] for node, lbl in X_labels_pred_clusters]\n",
    "    else:\n",
    "        cluster_labels_pred = [lbl for node, lbl in X_labels_pred_clusters]\n",
    "    \n",
    "    fig, axs = plt.subplots(1,2,figsize = (10,5), dpi = 96)\n",
    "\n",
    "    # Plot showing the predicted clusters formed\n",
    "    colors_list = cm.nipy_spectral(np.asarray(cluster_labels_pred).astype(float) / num_classes)\n",
    "    axs[0].scatter(*zip(*X_transform), marker='.', s=50, lw=0, alpha=0.7,\n",
    "                     c = colors_list, edgecolor='k')\n",
    "\n",
    "    for i, label in enumerate(X_labels):\n",
    "        x,y = X_transform[i]\n",
    "        if label in teachers_labels:\n",
    "            axs[0].annotate(\"TEACHER\", (x,y))\n",
    "    axs[0].set_title('Predicted clustering', fontsize=20)\n",
    "    \n",
    "    # Plot showing the actual clusters formed\n",
    "    cluster_labels_true = [lbl for node, lbl in X_labels_true_clusters]\n",
    "    colors_list = cm.nipy_spectral(np.asarray(cluster_labels_true).astype(float) / num_classes)\n",
    "    axs[1].scatter(*zip(*X_transform), marker='.', s=50, lw=0, alpha=0.7,\n",
    "                     c = colors_list, edgecolor='k')\n",
    "\n",
    "    for i, label in enumerate(X_labels):\n",
    "        x,y = X_transform[i]\n",
    "        if label in teachers_labels:\n",
    "            axs[1].annotate(\"TEACHER\", (x,y))\n",
    "    axs[1].set_title('True Clustering', fontsize=15)\n",
    "\n",
    "    fig.suptitle(in_file.rsplit('/',1)[-1].split('.')[0], fontsize=20)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create Hasse diagram\n",
    "g_hasse = simplex2hasse_uniform(data, max_order = None)\n",
    "\n",
    "# perform embedding with the below mentioned parameters\n",
    "n_walks = 1\n",
    "w_len = 100\n",
    "rw_bias = \"LOexp\"\n",
    "p_return = 1000000000.\n",
    "w2v_size = 100\n",
    "w2v_window = 10\n",
    "workers = 3\n",
    "\n",
    "s2v = Simplex2Vec.from_hasse_diagram(g_hasse, n_walks=n_walks, walk_length=w_len, rw_bias = rw_bias,\n",
    "                                        p = p_return, workers = workers, is_quiet = False)\n",
    "s2v_model = s2v.fit(size = w2v_size, window = w2v_window)\n",
    "\n",
    "# cluster the data \n",
    "X, X_labels_pred_clusters, X_labels_true_clusters = clusterer(s2v_model, num_classes, \n",
    "                                                              teachers_labels = teachers_labels)\n",
    "# produce NMI of the clustering\n",
    "NMI = get_NMI(X_labels_pred_clusters, X_labels_true_clusters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"NMI:\", NMI)\n",
    "max_acc_dict = get_max_accuracy_labels(X_labels_pred_clusters, X_labels_true_clusters)\n",
    "plot_clusterings(X, X_labels_pred_clusters, X_labels_true_clusters, teachers_labels = teachers_labels,\n",
    "                max_accuracy_dict=max_acc_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# USE CASE\n",
    "\n",
    "!! Currently under development !! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### EXAMPLE: create embeddings using increasingly higher orders without re-building hasse diag.\n",
    "\n",
    "\n",
    "#Build hasse diagram externally\n",
    "\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from Simplex2Vec.Simplex2Vec import Simplex2Vec\n",
    "\n",
    "from Simplex2Vec.utils import read_simplex_json\n",
    "from Simplex2Vec.simplex2hasse import *\n",
    "\n",
    "in_file = \"./data/sociopatterns_facets/aggr_5min_cliques_InVS15.json\"\n",
    "\n",
    "data = read_simplex_json(in_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasse_diag = simplex2hasse_proportional(data, max_order=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for order in range(1,4):\n",
    "    \n",
    "    s2v = Simplex2Vec.from_hasse_diagram(hasse_diag, n_walks=1, walk_length=10, hasse_max_order=order)\n",
    "    \n",
    "    print('\\nMAX ORDER = {}'.format(order))\n",
    "    print(nx.info(s2v.get_hasse_diagram(copy=False)))\n",
    "    \n",
    "    #model = s2v.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hasse_diag = simplex2hasse_LOadjusted(data, max_order=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for order in range(1,4):\n",
    "    \n",
    "    s2v = Simplex2Vec.from_hasse_diagram(hasse_diag, n_walks=1, walk_length=10, hasse_max_order=order)\n",
    "    \n",
    "    print('\\nMAX ORDER = {}'.format(order))\n",
    "    print(nx.info(s2v.get_hasse_diagram(copy=False)))\n",
    "    \n",
    "    #model = s2v.fit()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted distance matrix of brain data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from Simplex2Vec.Simplex2Vec import Simplex2Vec\n",
    "from Simplex2Vec.utils import read_data_HCP\n",
    "from os.path import join as osjoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ICAOrder = 15\n",
    "data_path = \"./data/HCP_Netmats/ICA\" + str(ICAOrder) \n",
    "#data_path = \"/home/jcwbillings/Documents/complexity72h/doublenegroni/data/HCP_PTN1200/netmats_3T_HCP1200_MSMAll_ICAd\" + str(ICAOrder) + \"_ts2/netmats/3T_HCP1200_MSMAll_d\" + str(ICAOrder) + \"_ts2\"\n",
    "filename = osjoin(data_path, 'Mnet2.pconn.nii')\n",
    "\n",
    "data = read_data_HCP(filename)              \n",
    "# HCP ICA data are delivered as gaussian distributed correlation z-stats. Proceed accordingly\n",
    "# Take caution for thresholds less than the 50th percentile, self-loops and negative correlations will be included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s2v = Simplex2Vec.from_graph(data, \n",
    "                               threshold=np.percentile(data,55), \n",
    "                               n_walks=10, \n",
    "                               walk_length=20, \n",
    "                               workers=1,\n",
    "                               hasse_max_order = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=s2v.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=s2v.refit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import literal_eval\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "X = []\n",
    "X_labels = []\n",
    "X_bNodes = []\n",
    "X_sizes = []\n",
    "for u in model.wv.vocab.keys():\n",
    "    X.append(model[u])\n",
    "    #X_labels.append(\"\".join(list(u)))\n",
    "    X_labels.append(u)\n",
    "    X_bNodes.append(literal_eval(X_labels[-1]))\n",
    "    X_sizes.append(np.size(X_bNodes[-1]))\n",
    "\n",
    "\n",
    "X_transform = TSNE(n_components=2).fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_clusters = 4\n",
    "\n",
    "clusterer = KMeans(n_clusters=n_clusters)\n",
    "cluster_labels = clusterer.fit_predict(X)\n",
    "clusterer_for_labels = KMeans(n_clusters=n_clusters, random_state=10)\n",
    "clusterer_for_labels_centers = clusterer_for_labels.fit_predict(X_transform)\n",
    "\n",
    "colors2 = cm.nipy_spectral(np.arange(n_clusters) / n_clusters)\n",
    "\n",
    "for hh in range(max(X_sizes)):\n",
    "\n",
    "    plt.figure(figsize = (12,9))\n",
    "    ax = plt.gca()\n",
    "    # 2nd Plot showing the actual clusters formed\n",
    "    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)\n",
    "    ax.scatter(*zip(*X_transform), marker='o', s=50, lw=0, alpha=0.7,\n",
    "                     c = colors, edgecolor='k')\n",
    "    \n",
    "    for i, label in enumerate(X_labels):\n",
    "        x,y = X_transform[i]\n",
    "        if X_sizes[i]==hh+1:\n",
    "            ax.annotate(label, (x,y))\n",
    "    ax.set_title(\"PCA embedding of the data into 2-dim space\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
